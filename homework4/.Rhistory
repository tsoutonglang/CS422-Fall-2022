setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4)
print("relationship, martial_status, capital_gain")
print("The first split was done on relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
model <- rpart(income ~ ., data=test.df, method="class")
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4)
print("What is the balanced accuracy of the model? (Note that in our test dataset, we have more observations of class “<=50” than we do of class “>50”. Thus, we are more interested in the balanced accuracy, instead of just accuracy. Balanced accuracy is calculated as the average of sensitivity and specificity.)")
#send in one variable to table there you get count for each class label for each variable sent into table
#send in two variables into table you get a confusion matrix
#table function
#names
#nrow
#working directory set from computer
setwd("/tmp")
#send in one variable to table there you get count for each class label for each variable sent into table
#send in two variables into table you get a confusion matrix
#table function
#names
#nrow
#working directory set from computer
setwd("/tmp")
#working directory set from computer
setwd("/tmp")
#send in one variable to table there you get count for each class label for each variable sent into table
#send in two variables into table you get a confusion matrix
#table function
#names
#nrow
#working directory set from computer
# setwd("/tmp")
data(hotel_bookings)
hotel_bookings
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4)
print("relationship, martial_status, capital_gain")
print("The first split was done on relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
print("What is the balanced accuracy of the model? (Note that in our test dataset, we have more observations of class “<=50” than we do of class “>50”. Thus, we are more interested in the balanced accuracy, instead of just accuracy. Balanced accuracy is calculated as the average of sensitivity and specificity.)")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
summary(model)
printcp(model)
print("capital_gain, education, and relationship")
printcp(model)
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
summary(model)
print("relationship, martial_status, capital_gain")
print("The first split was done on the relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
install.packages("caret")
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Model summary
summary(model)
# Print the cross validation results
printcp(model)
print("relationship, martial_status, capital_gain")
print("The first split was done on the relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
print("What is the balanced accuracy of the model? (Note that in our test dataset, we have more observations of class “<=50” than we do of class “>50”. Thus, we are more interested in the balanced accuracy, instead of just accuracy. Balanced accuracy is calculated as the average of sensitivity and specificity.)")
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
# Calculate sensitivity
sensitivity(factor(preds[,2]), factor(as.numeric(train$Fraud)))
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
# Calculate sensitivity
sensitivity(factor(preds[,2]), factor(as.numeric(train.df$income)))
View(preds)
View(test.df)
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
# Calculate sensitivity
sensitivity(as.factor(preds[,2]), factor(as.numeric(train.df$income)))
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
# Calculate sensitivity
sensitivity(as.factor(preds[,2]), as.factor(as.numeric(train.df$income)))
# Calculate sensitivity
sensitivity(factor(preds[,2]), as.factor(as.numeric(train.df$income)))
# Calculate sensitivity
sensitivity(preds[,2], as.factor(as.numeric(train.df$income)))
factor(
# Calculate sensitivity
sensitivity(as.factor(preds[,2]), as.factor(as.numeric(train.df$income)))
# Calculate specificity
specificity(factor(preds[,2]), factor(as.numeric(train.df$income)))
# Calculate sensitivity
sensitivity(as.factor(preds[,2]), as.factor(as.numeric(train.df$income)))
# Calculate sensitivity
sensitivity(as.factor(preds[,2]), as.factor(train.df$income))
# Calculate predictions
preds <- as.factor(predict(model, train.df))
trainFactor <- as.factor(train.df$income)
# Calculate predictions
preds <- as.factor(predict(model, train.df))[,2]
preds <- preds[,2]
preds <- preds[,2]
# Calculate predictions
preds <- predict(model, train.df)
preds <- preds[,2]
preds <- as.factor(preds)
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds)
# Calculate predictions
preds <- predict(model, train.df)
View(preds)
View(train.df)
trainFactor <- as.factor(train.df$income)
preds <- as.factor(preds[,])
preds
trainFactor
preds <- as.factor(preds[,2])
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[,2])
preds
preds <- as.factor(preds[1,])
preds <- as.factor(preds[:])
preds <- as.factor(preds[1,2])
preds <- as.factor(preds[0,2])
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[0,2])
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[0,])
preds
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[1,])
preds
# Calculate sensitivity
sensitivity(preds, trainFactor)
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[1,])
trainFactor <- as.factor(train.df$income)
# Calculate sensitivity
sensitivity(preds, trainFactor)
trainFactor <- as.factor(train.df$income)
trainFactor
model
# Calculate predictions
preds <- predict(model, train.df)
library(caret)
# Calculate predictions
preds <- predict(model, train.df)
preds <- as.factor(preds[1,])
trainFactor <- as.factor(train.df$income)
trainFactor
# Calculate sensitivity
sensitivity(preds, trainFactor)
confusionMatrix(predictions, train.df$income)
library(caret)
# # Calculate predictions
predictions <- predict(model, train.df)
# predictions <- as.factor(preds[1,])
# trainFactor <- as.factor(train.df$income)
# trainFactor
#
# # Calculate sensitivity
# sensitivity(preds, trainFactor)
#
# # Calculate specificity
# specificity(factor(preds[,2]), factor(as.numeric(train.df$income)))
confusionMatrix(predictions, train.df$income)
View(predictions)
income <- train.df %>% select(income)
View(income)
View(predictions)
# Confusion matrix
predictions <- predict(model,test.df,type="class")
model <- rpart(income ~ ., data=train.df, method="class")
```{r}
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Confusion matrix
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,test.df$income)
confusionMatrix(predictions,as.factor(test.df$income))
# Print the cross validation results
printcp(model)
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Confusion matrix
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="1994 U.S. Census")
confusionMatrix(predictions,as.factor(test.df$income))
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
printcp(model)
confusionMatrix(predictions,as.factor(test.df$income))
printcp(model)
summary(model)
confusionMatrix(predictions,as.factor(test.df$income))
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
cat(print("sensitivity: 0.9478\nSpecificity: 0.5061"))
sensitivity <- 0.9478
specificity <- 0.5061
cat(print("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity))
sensitivity <- 0.9478
specificity <- 0.5061
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity))
balAcc <- (sensitivity + specificity) / 2
sensitivity <- 0.9478
specificity <- 0.5061
balAcc <- (sensitivity + specificity) / 2
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity, "\nBalanced Accuracy: ", balAcc))
balAcc <- round((sensitivity + specificity) / 2, digits = 3)
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity, "\nBalanced Accuracy: ", balAcc))
balErr <- 1 - balAcc
paste0("Balanced Error: ", balErr)
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity))
cat(paste0("Sensitivity: ", round(sensitivity, digits = 3), "\nSpecificity: ", round(specificity, digits = 3)))
library(pROC)
install.packages("pROC")
rocScore <- roc(test.df, predictions)
library(pROC)
rocScore <- roc(test.df, predictions)
library(ROCR)
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
pruned <- prune(model, cp = bestcp)
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Confusion matrix
printcp(model)
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="1994 U.S. Census")
print("relationship, martial_status, capital_gain")
print("The first split was done on the relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
library(caret)
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
sensitivity <- 0.9478
specificity <- 0.5061
balAcc <- round((sensitivity + specificity) / 2, digits = 3)
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity, "\nBalanced Accuracy: ", balAcc))
balErr <- 1 - balAcc
paste0("Balanced Error: ", balErr)
cat(paste0("Sensitivity: ", round(sensitivity, digits = 3), "\nSpecificity: ", round(specificity, digits = 3)))
library(ROCR)
library(rpart.plot)
bestcp <- model$cptable[which.min(model$cptable[,"xerror"]),"CP"]
pruned <- prune(model, cp = bestcp)
val1 = predict(pruned, train.df, type = "prob")
pred_val <-prediction(val1[,2],val$income)
pred_val <-prediction(val1[,2],train.df$income)
perf_val <- performance(pred_val,"auc")
plot(performance(pred_val, measure="lift", x.measure="rpp"), colorize=TRUE)
predict.df <- data.frame(predictions, train.df$income)
predict.df <- data.frame(predictions, test.df$income)
View(predict.df)
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(predict.df$predictions, predict.df$test.df.income)
View(predict.df)
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Confusion matrix
printcp(model)
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="1994 U.S. Census")
print("relationship, martial_status, capital_gain")
print("The first split was done on the relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
library(caret)
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
sensitivity <- 0.9478
specificity <- 0.5061
balAcc <- round((sensitivity + specificity) / 2, digits = 3)
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity, "\nBalanced Accuracy: ", balAcc))
balErr <- 1 - balAcc
paste0("Balanced Error: ", balErr)
cat(paste0("Sensitivity: ", round(sensitivity, digits = 3), "\nSpecificity: ", round(specificity, digits = 3)))
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(predict.df$predictions, predict.df$test.df.income)
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(as.numeric(predict.df$predictions), as.numeric(predict.df$test.df.income))
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(as.numeric(predict.df$predictions), predict.df$test.df.income)
perf <- performance(perd, "tpr","fpr")
plot(perf, colorize=TRUE)
auc.tmp <- performance(perd,"auc")
print(as.numeric(auc.tmp@y.values))
print(round(as.numeric(auc.tmp@y.values), digits = 3))
print("AUC: ",round(as.numeric(auc.tmp@y.values), digits = 3))
paste0("AUC: ", round(as.numeric(auc.tmp@y.values), digits = 3))
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(as.numeric(predict.df$predictions), predict.df$test.df.income)
auc.tmp <- performance(perd,"auc")
paste0("AUC: ", round(as.numeric(auc.tmp@y.values), digits = 3))
perf <- performance(perd, "tpr","fpr")
plot(perf, colorize=TRUE)
printcp(model)
set.seed(1122)
table(train.df$income)
uSamp.df <- sample(train.df$income("<=50K"))
set.seed(1122)
table(train.df$income)
uSamp.df <- sample(train.df$income, size=7650)
uSamp.df <- sample(train.df$income, size=7650)
uSamp.df
set.seed(1122)
income.df <- train.df$income
table(income.df)
uSamp.df <- sample(income.df$"<=50K", size=7650)
uSamp.df <- sample(income.df[1], size=7650)
uSamp.df <- sample(income.df[1,], size=7650)
uSamp.df <- sample(income.df, size=7650)
uSamp.df
set.seed(1122)
table(train.df$income)
uSamp.df <- separate(train.df, income)
library(tidyr)
uSamp.df <- separate(train.df, income)
library(tidyr)
uSamp.df <- separate(train.df, income, into = NA)
library(tidyr)
uSamp.df <- separate(train.df, income(">50K", "<=50K"))
# library(tidyr)
new_income <- sapply(train.df, unclass) %>%
uSamp.df <- sample(income.df, size=7650)
# library(tidyr)
new_income <- sapply(train.df, unclass) %>%
# library(tidyr)
new_income <- sapply(train.df, unclass) %>%
na.omit()
# library(tidyr)
new_income <- sapply(train.df, unclass)
View(auc.tmp)
View(income)
View(new_income)
# library(tidyr)
new_income <- sapply(train.df, unclass)
na.omit()
# library(tidyr)
new_income <- sapply(train.df, unclass)
sample(1:nrow(new_income), 0.90*dim(new_income)[1])
uSamp.df <- sample(train.df$income, size=7650)
uSamp.df
uSamp.df <- sample(train.df$income[1], size=7650)
uSamp.df <- sample(train.df$income[0], size=7650)
uSamp.df <- sample(train.df$income[2], size=7650)
uSamp.df <- sample(train.df$income[0], size=7650)
income.df <- train.df$income
set.seed(1122)
table(train.df$income)
income.df <- train.df$income
uSamp.df <- sample(income.df, size=7650)
income.df
View(train.df)
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
uSamp.df <- sample(train.df$income == '<=50K', size=7650)
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
uSamp.df <- sample(train.df[train.df$income == '<=50K'], size=7650)
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
# uSamp.df <- sample(train.df[train.df$income == '<=50K'], size=7650)
train.df[train.df$income == '<=50K']
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
# uSamp.df <- sample(train.df[train.df$income == '<=50K'], size=7650)
train.df[train.df$income == '<=50K']
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
# uSamp.df <- sample(train.df[train.df$income == '<=50K'], size=7650)
train.df[train.df$income == "<=50K"]
setwd("C:\\Users\\tsout\\OneDrive\\Desktop\\cs422\\CS422-labs\\homework4")
library(dplyr)
library(rpart)
library(rpart.plot)
train.df <- read.csv("adult-train.csv", header=TRUE, comment="#")
test.df <- read.csv("adult-test.csv", header=TRUE, comment="#")
# Cleaning training data set
sum(train.df$occupation == "?")
trainClean <- which(train.df$occupation == "?")
train.df <- slice(train.df, -trainClean)
sum(train.df$occupation == "?")
# cleaning testing data set
sum(test.df$occupation == "?")
testClean <- which(test.df$occupation == "?")
test.df <- slice(test.df, -testClean)
sum(test.df$occupation == "?")
set.seed(1122)
model <- rpart(income ~ ., data=train.df, method="class")
# Confusion matrix
summary(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="1994 U.S. Census")
print("relationship, martial_status, capital_gain")
print("The first split was done on the relationship predictor. The left side has 16,578 observations and the right side has 14,139 observations.")
library(caret)
predictions <- predict(model,test.df,type="class")
confusionMatrix(predictions,as.factor(test.df$income))
sensitivity <- 0.9478
specificity <- 0.5061
balAcc <- round((sensitivity + specificity) / 2, digits = 3)
cat(paste0("Sensitivity: ", sensitivity, "\nSpecificity: ", specificity, "\nBalanced Accuracy: ", balAcc))
balErr <- 1 - balAcc
paste0("Balanced Error: ", balErr)
cat(paste0("Sensitivity: ", round(sensitivity, digits = 3), "\nSpecificity: ", round(specificity, digits = 3)))
library(ROCR)
predict.df <- data.frame(predictions, test.df$income)
perd <- prediction(as.numeric(predict.df$predictions), predict.df$test.df.income)
auc.tmp <- performance(perd,"auc")
paste0("AUC: ", round(as.numeric(auc.tmp@y.values), digits = 3))
perf <- performance(perd, "tpr","fpr")
plot(perf, colorize=TRUE)
printcp(model)
set.seed(1122)
table(train.df$income)
income.df <- train.df$income
# income.d[c('<=50K', '>50K')] <- str_split_fixed(df$Name, ' ', 2)
# uSamp.df <- sample(train.df[train.df$income == '<=50K'], size=7650)
train.df[train.df$income == "<=50K"]
